=========================================================
1)select date, temperature from temperature_data where zip_code>=300000 and zip_code<399999;
OK
1990-03-10 00:00:00	15
1991-01-10 00:00:00	22
1990-02-12 00:00:00	9
1991-03-10 00:00:00	16
1990-01-10 00:00:00	23
1991-02-12 00:00:00	10
1993-03-10 00:00:00	16
1994-01-10 00:00:00	23
1991-02-12 00:00:00	10
1991-03-10 00:00:00	16
1990-01-10 00:00:00	23
1991-02-12 00:00:00	10
==============================================================
2)select year(date),MAX(temperature) from temperature_data group by year(date);
Query ID = acadgild_20170117163232_7e1b4be4-bb8d-4377-93c5-5a6732446a07
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1484635641014_0005, Tracking URL = http://localhost:8088/proxy/application_1484635641014_0005/
Kill Command = /home/acadgild/hadoop-2.6.0/bin/hadoop job  -kill job_1484635641014_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2017-01-17 16:33:05,116 Stage-1 map = 0%,  reduce = 0%
2017-01-17 16:33:26,593 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.31 sec
2017-01-17 16:33:54,155 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.36 sec
MapReduce Total cumulative CPU time: 8 seconds 360 msec
Ended Job = job_1484635641014_0005
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.38 sec   HDFS Read: 830 HDFS Write: 32 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 380 msec
OK
1990	23
1991	22
1993	16
1994	23
==============================================================
3)select year(date),MAX(temperature) from temperature_data group by year(date) having count(year(date))>2;  
Query ID = acadgild_20170117164040_bb9d3442-ebe7-4734-b944-576d1e737452
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1484635641014_0006, Tracking URL = http://localhost:8088/proxy/application_1484635641014_0006/
Kill Command = /home/acadgild/hadoop-2.6.0/bin/hadoop job  -kill job_1484635641014_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2017-01-17 16:40:46,381 Stage-1 map = 0%,  reduce = 0%
2017-01-17 16:41:01,891 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.8 sec
2017-01-17 16:41:27,740 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.37 sec
MapReduce Total cumulative CPU time: 7 seconds 370 msec
Ended Job = job_1484635641014_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 7.37 sec   HDFS Read: 830 HDFS Write: 16 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 370 msec
OK
1990	23
1991	22
Time taken: 63.287 seconds, Fetched: 2 row(s)
==============================================
4)INSERT OVERWRITE LOCAL DIRECTORY 'tmp/output'
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY '|'
    > SELECT * FROM temperature_data_vw;
Query ID = acadgild_20170117170000_e3f96b4a-ffe5-4ff0-b921-d4dcabfdb32d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1484635641014_0007, Tracking URL = http://localhost:8088/proxy/application_1484635641014_0007/
Kill Command = /home/acadgild/hadoop-2.6.0/bin/hadoop job  -kill job_1484635641014_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2017-01-17 17:00:35,944 Stage-1 map = 0%,  reduce = 0%
2017-01-17 17:00:55,014 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.34 sec
2017-01-17 17:01:20,240 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 6.59 sec
2017-01-17 17:01:25,901 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 10.39 sec
MapReduce Total cumulative CPU time: 10 seconds 390 msec
Ended Job = job_1484635641014_0007
Copying data to local directory tmp/output
Copying data to local directory tmp/output
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 10.39 sec   HDFS Read: 830 HDFS Write: 16 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 390 msec
OK
Time taken: 81.564 seconds


output:
=====================
cd /tmp/output

1990|23
1991|22


